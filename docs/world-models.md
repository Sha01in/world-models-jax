# World Models
**Authors:** David Ha, Jürgen Schmidhuber
**Date:** May 2018 (arXiv:1803.10122v4)

## Abstract
We explore building generative neural network models of popular reinforcement learning environments. Our *world model* can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.

---

## 1. Introduction
Humans develop a mental model of the world based on what they are able to perceive with their limited senses. The decisions and actions we make are based on this internal model. Jay Wright Forrester described a mental model as: *The image of the world around us, which we carry in our head, is just a model... He has only selected concepts, and relationships between them, and uses those to represent the real system.*

To handle the vast amount of information that flows through our daily lives, our brain learns an abstract representation of both spatial and temporal aspects of this information. Evidence suggests that what we perceive at any given moment is governed by our brain’s prediction of the future based on our internal model.

In this work, we build probabilistic generative models of OpenAI Gym environments. We divide the agent into a large world model and a small controller model. We first train a large neural network to learn a model of the agent’s world in an unsupervised manner, and then train the smaller controller model to learn to perform a task using this world model.

---

## 2. Agent Model
The agent consists of three components that work closely together: **Vision (V)**, **Memory (M)**, and **Controller (C)**.

### 2.1. VAE (V) Model
The environment provides a high-dimensional input observation at each time step (usually a 2D image frame). The role of the V model is to learn an abstract, compressed representation of each observed input frame.
We use a Variational Autoencoder (VAE) as the V model. It compresses each image frame into a small latent vector $z$.

### 2.2. MDN-RNN (M) Model
While V compresses what the agent sees at each time frame (spatial), we also want to compress what happens over time (temporal). The M model serves as a predictive model of the future $z$ vectors that V is expected to produce.

Since complex environments are stochastic, we model $P(z)$ as a probability density function rather than a deterministic prediction. We use a **Recurrent Neural Network (RNN)** combined with a **Mixture Density Network (MDN)** output layer.
*   The RNN models $P(z_{t+1} | a_t, z_t, h_t)$.
*   $a_t$: Action taken at time $t$.
*   $h_t$: Hidden state of the RNN at time $t$.
*   During sampling, a **temperature parameter $\tau$** can be adjusted to control model uncertainty.

### 2.3. Controller (C) Model
The Controller (C) is responsible for determining the actions to maximize cumulative reward. It is deliberately made as simple and small as possible. C is a single layer linear model that maps $z_t$ and $h_t$ directly to action $a_t$:

$$ a_t = W_c [z_t, h_t] + b_c $$

Where $W_c$ and $b_c$ are the weight matrix and bias vector. $z_t$ and $h_t$ are concatenated.

### 2.4. Putting V, M, and C Together
**Pseudocode for the Agent in OpenAI Gym:**

```python
def rollout(controller):
    ''' env, rnn, vae are global variables '''
    obs = env.reset()
    h = rnn.initial_state()
    done = False
    cumulative_reward = 0
    while not done:
        # 1. Encode observation to latent z
        z = vae.encode(obs)
        
        # 2. Controller decides action based on z and hidden state h
        a = controller.action([z, h])
        
        # 3. Step the environment
        obs, reward, done = env.step(a)
        cumulative_reward += reward
        
        # 4. Update RNN hidden state (prediction for next step)
        h = rnn.forward([a, z, h])
        
    return cumulative_reward
```

**Optimization:**
We train V and M effectively with backpropagation. To optimize C, we use **Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)** since the parameter space of C is small.

---

## 3. Car Racing Experiment
**Environment:** `CarRacing-v0` (top-down driving).
**Goal:** Visit as many tiles as possible.
**Input:** Pixel inputs.
**Actions:** 3 continuous (Steering, Acceleration, Brake).

### 3.1 Procedure
1.  **Collect Data:** Collect 10,000 rollouts from a random policy. Record actions $a_t$ and observations.
2.  **Train V:** Train VAE to encode frames into $z \in \mathbb{R}^{32}$. Minimize reconstruction loss + KL divergence.
3.  **Train M:** Pre-process frames into $z_t$. Train MDN-RNN to model $P(z_{t+1} | a_t, z_t, h_t)$ as a mixture of Gaussians.
4.  **Train C:** Define Controller $a_t = W_c [z_t, h_t] + b_c$. Use CMA-ES to solve for weights maximizing expected reward.

### 3.2 Results (Scores over 100 trials)
*   **V Model Only (Hidden layer C, no M):** $788 \pm 141$
*   **Full World Model (V + M + Linear C):** **$906 \pm 21$**
*   *Note:* Solving the task is defined as a score of 900.

### 3.3 Car Racing Dreams
We can train the agent entirely inside the "dream" (hallucination) generated by M. The M model can produce the probability distribution of $z_{t+1}$, sample a $z_{t+1}$, and feed it back as the "observation."

---

## 4. VizDoom Experiment
**Environment:** VizDoom "Take Cover".
**Goal:** Survive as long as possible (avoid fireballs).
**Difference:** M must also predict whether the agent dies (`done` signal).

### 4.1 Procedure
1.  **Collect Data:** 10,000 random rollouts.
2.  **Train V:** Encode frames to $z \in \mathbb{R}^{64}$.
3.  **Train M:** Model $P(z_{t+1}, d_{t+1} | a_t, z_t, h_t)$, where $d$ is the binary "done" event.
4.  **Train C inside Dream:** Train C using CMA-ES *entirely* inside the virtual environment generated by M.
5.  **Transfer:** Use the learned policy on the actual environment.

### 4.2 Training Inside the Dream
Training in the hallucination allows us to vary the **temperature $\tau$**.
*   Increasing $\tau$ increases uncertainty/randomness in the dream.
*   We found agents trained with higher $\tau$ generally perform better in the real (cleaner) environment.
*   This helps prevent the agent from finding "adversarial policies" that exploit imperfections in the World Model (e.g., moving in a specific way that prevents the RNN from generating fireballs).

### 4.3 Results (Take Cover)
*   Random Policy: $210 \pm 108$
*   Gym Leader: $820 \pm 58$
*   **World Model ($\tau=1.15$):** **$1092 \pm 556$**

---

## 5. Iterative Training Procedure
For more complex tasks, a random policy might not explore enough of the world to build a good model. An iterative approach (similar to "Learning to Think", Schmidhuber 2015a) can be used:
1.  Initialize M, C randomly.
2.  Rollout to actual environment $N$ times. Save data.
3.  Train M on saved data.
4.  Train C to optimize expected rewards *inside of M*.
5.  Go back to step 2.

(Note: For the experiments in this paper, one iteration was sufficient, but this framework enables harder tasks).

---

# Appendix: Technical Implementation Details
*This section is critical for code reproduction.*

## A.1. Variational Autoencoder (V Model)
**Architecture:** Convolutional VAE (ConvVAE).
**Input:** Images resized to $64 \times 64 \times 3$.
**Latent Vector:** Sampled from Gaussian prior $N(\mu, \sigma I)$.
*   Car Racing: $N_z = 32$
*   VizDoom: $N_z = 64$

**Encoder Layers:**
1.  `Conv 32 filters, 4x4 kernel, stride 2, ReLU`
2.  `Conv 64 filters, 4x4 kernel, stride 2, ReLU`
3.  `Conv 128 filters, 4x4 kernel, stride 2, ReLU`
4.  `Conv 256 filters, 4x4 kernel, stride 2, ReLU`
5.  Global Flatten -> Dense to $2 \times N_z$ (outputs $\mu$ and log $\sigma^2$).

**Decoder Layers:**
1.  Dense input $z$ -> Reshape to $1 \times 1 \times 1024$.
2.  `Deconv 128 filters, 5x5 kernel, stride 2, ReLU`
3.  `Deconv 64 filters, 5x5 kernel, stride 2, ReLU`
4.  `Deconv 32 filters, 6x6 kernel, stride 2, ReLU`
5.  `Deconv 3 filters, 6x6 kernel, stride 2, Sigmoid` (Output image).

**Training:**
*   Trained for 1 epoch on data collected from random policy.
*   Loss: Reconstruction loss ($L2$ distance) + KL divergence.

## A.2. Recurrent Neural Network (M Model)
**Architecture:** LSTM combined with Mixture Density Network (MDN).
**Hidden Units:**
*   Car Racing: 256 hidden units.
*   VizDoom: 512 hidden units.
**MDN Output:**
*   Modeled as a mixture of **5 Gaussians**.
*   Outputs $\mu$, $\sigma$, and mixing coefficients $\pi$ for the next $z$.
*   We do *not* model the correlation parameter $\rho$; we assume a diagonal covariance matrix.
*   For VizDoom, it also outputs probability of `done` (death). If prob > 0.5, episode ends in dream.

**Training:**
*   Trained for 20 epochs.
*   Uses **Teacher Forcing** (inputs are recorded $z$ and $a$).

## A.3. Controller (C Model)
**Architecture:** Single linear layer.
**Input:** Concatenation of $[z_t, h_t]$.
*   $z_t$: Current latent vector.
*   $h_t$: Current hidden state of LSTM.
**Output:** Action vector $a_t$.
*   We apply `tanh` to clip and bound actions.
*   Car Racing: 3 actions (Steering [-1,1], Gas [0,1], Brake [0,1]).
*   VizDoom: Discrete actions converted to continuous space.

## A.4. Evolution Strategies (Optimizer)
**Algorithm:** CMA-ES (Covariance-Matrix Adaptation Evolution Strategy).
**Population Size:** 64.
**Rollouts:** Each agent performs the task 16 times (averaging the cumulative reward) to reduce variance.
**Generations:**
*   Car Racing: ~1800 generations.
*   VizDoom: ~2000 generations.

## A.5 Computational Notes
*   Training V and M takes less than an hour on a single GPU.
*   Evolution of C runs on CPUs in parallel.
*   For VizDoom, "DoomRNN" (training inside dream) is computationally efficient as it avoids rendering frames, operating only on latent vectors $z$.